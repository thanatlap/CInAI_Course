{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Training Deep Recurrent Neural Network - Part 2\n",
    "\n",
    "Name1, Student's ID1<br>\n",
    "Name2, Student's ID2<br>\n",
    "Name3, Student's ID3<br>\n",
    "\n",
    "**Note: Please name your file**\n",
    "\n",
    "## Lab Instruction - Language Modelling and Text Generation\n",
    "\n",
    "In this lab, you will learn to train a deep recurrent neural network using LSTM with the Keras library using the Tensorflow backend. Your task is to implement the natural language modelling and text generation.\n",
    "\n",
    "Select your favourite book from https://www.gutenberg.org/browse/scores/top and download it as a text file. Then, you will train your language model using RNN-LSTM. \n",
    "\n",
    "- Language model (in Thai): http://bit.ly/language_model_1\n",
    "- Tutorial on how to create a language model (in English): https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n",
    "\n",
    "To evaluate the model, the perplexity measurement is used: https://stats.stackexchange.com/questions/10302/what-is-perplexity\n",
    "\n",
    "Last, fine-tune your model. You have to try different hyperparameter or adding more data. Discuss your result.\n",
    "\n",
    "\n",
    "\n",
    "**The total lab score is 20 which will be evaluated as follows:**</br>\n",
    "1. Specification (Do as the instruction said. This include the model tuning section where you have to do a proper amount of tuning) - 10 points\n",
    "2. Design of logic (No weired things in the process) - 5 points\n",
    "3. Journaling (Communicate your thought process, comment your code, and discuss result & analyse **in every step**) - 5 points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load your data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing \n",
    "\n",
    "*Note that only story will be used as a dataset, footnote and creddit are not include.*\n",
    "\n",
    "The symbol '\\n' is indicated the end of the line ``<EOS>``, which is for our model to end the sentence here.\n",
    "\n",
    "To create a corpus for your model. The following code is can be used:</br>\n",
    "*Note that other techniques can be used*\n",
    "\n",
    "```python\n",
    "# cut the text in semi-redundant sequences of maxlen characters.\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "```\n",
    "\n",
    "The code loop through the data from first word to the last word. The maxlen define a next n word for a model to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "# Create corpus & Word vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Language Model\n",
    "\n",
    "Define RNN model using LSTM and word embedding representation</br>\n",
    "We will used perplexity as a metrics\n",
    "\n",
    "```python\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = keras.backend.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = keras.backend.pow(2.0, cross_entropy)\n",
    "    return perplexity\n",
    "```\n",
    "\n",
    "To used custom metrics function > https://keras.io/metrics/\n",
    "\n",
    "For a loss function `categorical_crossentropy` is used, any optimzation method can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluate your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model using perplexity measurment (You can try adding other measurements as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Text generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, max_sequence_len, tolenizer):\n",
    "    # Loop through the next n words\n",
    "    for _ in range(200):\n",
    "        # Preprecess your seed_text and predict the output\n",
    "        # ======\n",
    "        # Add your code here\n",
    "        # ======\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            # convert word vector representation to a word string\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "        if \n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate your sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Model Tuning \n",
    "\n",
    "Write down why you design this architecture or why you choose this set of parameter</br>\n",
    "You should have at least 1 different architectures/set of hyperparameters per person in your team</br>\n",
    "Last, train your best performed model **on 50 epoch** (or you can try 100 epoch but this will take time)</br>\n",
    "*Note: For the last step, please turn off a verbose during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out different hyperparameter & model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Help your model to generate a short story \n",
    "\n",
    "**Example** https://medium.com/deep-writing/harry-potter-written-by-artificial-intelligence-8a9431803da6\n",
    "\n",
    "Write your result in a `markdown` cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your short-story from your model (Add your creativity here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Natural language Processing and Language model\n",
    "1. https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e \n",
    "2. https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b\n",
    "3. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "**Music generates by RNN**\n",
    "https://soundcloud.com/optometrist-prime/recurrence-music-written-by-a-recurrent-neural-network\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
