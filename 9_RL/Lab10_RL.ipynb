{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab10 Introduction to Reinforcement Learning\n",
    "\n",
    "- Name, Student ID 1\n",
    "- Name, Student ID 2\n",
    "\n",
    "## Lab Instruction\n",
    "\n",
    "This lab, we will implement an Q-learning agent that will solve the GridWorld problem. The objective of your agent is to maximize the reward by find the shortest path to the exit without stepping on a bomb or a cliff. The result will be compare with the randomly move agent.\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "The reinforcement learning components:</br>\n",
    "<img src='img/rl.png' width=500>\n",
    "\n",
    "- Reinforcement Learning Blog Post (Thai): <a href=\"https://medium.com/asquarelab/ep-1-reinforcement-learning-%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B9%89%E0%B8%99-acfa9d42394c\"> Thammasorn, A-Square</a></br>\n",
    "- Reinforcement Learning Blog Post (Eng): <a href=\"http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/\"> Adventures in Machine Learning</a>\n",
    "\n",
    "### Temporal-Different Learning\n",
    "**Temporal-difference (TD) learning** is a combination of **Monte Carlo** ideas and **dynamic programming (DP)** ideas.</br>\n",
    "Like Monte Carlo methods, TD methods **can learn directly from raw experience without a model of the environmentâ€™s dynamics**.</br> \n",
    "Like DP, TD methods update estimates based in part on other learned estimates, **without waiting for a final outcome** (they bootstrap).</br>\n",
    "The relationship between TD, DP, and Monte Carlo methods is a recurring theme in the theory of reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment - Cliff Gridworld\n",
    "\n",
    "The environment is a Cliff Gridworld, illustrated as follows: </br>\n",
    "\n",
    "<img src='img/grid.png' width=500></br>\n",
    "\n",
    "The world is a 10x10 grid where the exit is at bottom right corner. The bomb is randomly generated around the exit.</br>\n",
    "The agent is randomly start around the top left corner section. </br>\n",
    "Each step count as -1. In other word, the more step your agent take, the more penalty you have.</br>\n",
    "If your agent move against the corner, your agent will move. If your agent step to the cliff, the penalty is -20, same as stepping into a bomb which get -10 penalty.</br>\n",
    "If your agent reach to the exit, you will recieve 20 reward.</br>\n",
    "\n",
    "The episode is end when your agent found a way out and get a reward or step on a bomb and get the penalty point.</br>\n",
    "Your agent is at the starting point when the new episode begin.\n",
    "\n",
    "```env.World```</br>\n",
    ">**Properties**\n",
    "- height: The height of a grid world\n",
    "- width: The width of a grid world\n",
    "- current_location: current location of your agent.\n",
    "- actions: a list of an available actions (up,down,left,right)</br>\n",
    "\n",
    ">**Methods**\n",
    "- ```available_actions()``` Get a set of available actions\n",
    "- ```move_agent(action)``` Move an agent to the given direction and return a reward of that action.\n",
    "- ```reset()``` Reset the state of the environment to the starting point.\n",
    "- ```end_state()``` Get the end state. Return True if the state is end. False, otherwise.\n",
    "- ```render()``` Show the current terran of the grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = env.World()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Random Agent\n",
    "\n",
    "Define an agent that walk randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Random Agent Object\n",
    "class DumbAgent():\n",
    "    def action(self, available_actions):\n",
    "        #\n",
    "        # Code Here\n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code\n",
    "agent = DumbAgent()\n",
    "agent.action([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define a Q-Agent\n",
    "\n",
    "Define an agent that used q-learning using the following pseudocode: </br>\n",
    "<img src='img/q_learning.png'>\n",
    "\n",
    "- S is a state of the environment\n",
    "- S' is a next state\n",
    "- A is a action choosen by the agent\n",
    "- A' is a next action\n",
    "- epsilon, alpha, gamma are the parameter for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent():\n",
    "    \n",
    "    def __init__(## Code here ##):\n",
    "        #\n",
    "        # Code here\n",
    "        #\n",
    "    def action(self, available_actions):\n",
    "        #\n",
    "        # Code here\n",
    "        #\n",
    "    def update( ## Code here ##):\n",
    "        #\n",
    "        # Code here\n",
    "        #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Deploy Your Agent\n",
    "\n",
    "Create a function to run your agent in an environment. Your agent will run n trails, each trails has a maximum of m max_steps_per_episode.</br>\n",
    "For the Q-agent, you have to update Q-value for your agent using following code: </br>\n",
    "```python\n",
    "if learn: \n",
    "    agent.update( ... )\n",
    "```\n",
    "\n",
    "The function must record the total reward in each trials and return when the function end.\n",
    "```python\n",
    "total_reward = run(env, agent, trials, max_steps_per_episode, learn=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, agent, trials, max_steps_per_episode, learn=False):\n",
    "    #\n",
    "    # Code here\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward\n",
    "def plot_total_reward(total_reward):\n",
    "    plt.plot(total_reward)\n",
    "    plt.title('The Sum of Reward During Each Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Dump Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Discussion\n",
    "\n",
    "Compare the result from a dump agent, as a based line, and the q-agent with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your discuss as a Markdown Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Q-Table\n",
    "\n",
    "Show the agent's Q-table using to make a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viz_q_table(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            Viz_q_table(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Viz_q_table(agentQ.q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
